#cloud-config

hostname: InstallationHostname

# include one or more SSH public keys
ssh_authorized_keys:
  - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQC7DVGV5Agh45zZFJHjpUWjRgB1u5JuypitHOScLgm6i/tPiLjslbVOma5M0XMt9b5xz+STtRwlqH2vax+vYpwTIExSrlJmNKez/6KJJ9Xim6DEbkLX7FWYj4nr5/YNhAH6SLILtFdvbSVE6PAT92gYvDPtSEs57SdUK/W4Y+XWQXV7RtAcSEahKEvo5/eqFMtYnQNlOIu7gaKZSgjMfHTl8aokwRmACsG/OnOSvfXwGJcNoId1H78e9X4S6QeOz92h3D3VRrPCtiJSnceZ0xMJe/R1m3z7uhVJQTGQH/WUSHPNKRwDEehvCP13bBQ6ok+s3pk1VYFeIGABxIo0evStRSfKBXa98lUleyfpFYaPUmyhw6STgeKp+dA58NzJySS3mJQlde8x1MtQmRsomqI9vio9KxsYGMJmO0W77I7MX8enob1x6OP9dJySvOnY7fF62UR9CeTQFrxBS/tL10Xr4O3/3zYdX3mmdYI7ZhDPuEqH19aqadNkDFDFJzRa1rwfXBsSZkxgCdhP55ARnEeHYFEy5oyxxBcFfNB56zfw53cdcCGQA9oqmZEoAto/m54I7bPgAUebJHzcWDNrOjOhdeG9dpMVaaOphuFlXIx0lkvoOWu6Tr/PTnhQe8JCR3xQ6y6UiIQ1iA2gNPvJsy/p0BSlweKvS8Csdb0ZUe1qJw== jaohaohsuan@gmail.com

write_files:
  - path: /srv/kubeadm/master.yaml
    owner: root
    permissions: "0644"
    content: |
      apiVersion: kubeadm.k8s.io/v1alpha1
      kind: MasterConfiguration
      api:
        advertiseAddresses:
        - ServerIPAddress
      etcd:
        endpoints:
        - http://ServerIPAddress:2379
      kubernetesVersion: K8SVersion
      secrets:
        givenToken: KubernetesToken
  - path: /srv/asset/manifests/calico.yaml
    owner: root
    permissions: "0644"
    content: |
      # This ConfigMap is used to configure a self-hosted Calico installation.
      # http://docs.projectcalico.org/v2.0/getting-started/kubernetes/installation/hosted/kubeadm/
      kind: ConfigMap
      apiVersion: v1
      metadata:
        name: calico-config
        namespace: kube-system
      data:
        # The location of your etcd cluster.  This uses the Service clusterIP
        # defined below.
        etcd_endpoints: "http://ServerIPAddress:2379"

        # True enables BGP networking, false tells Calico to enforce
        # policy only, using native networking.
        enable_bgp: "true"

        # The CNI network configuration to install on each node.
        cni_network_config: |-
          {
              "name": "k8s-pod-network",
              "type": "calico",
              "etcd_endpoints": "__ETCD_ENDPOINTS__",
              "log_level": "info",
              "ipam": {
                  "type": "calico-ipam"
              },
              "policy": {
                  "type": "k8s",
                   "k8s_api_root": "https://__KUBERNETES_SERVICE_HOST__:__KUBERNETES_SERVICE_PORT__",
                   "k8s_auth_token": "__SERVICEACCOUNT_TOKEN__"
              },
              "kubernetes": {
                  "kubeconfig": "/etc/cni/net.d/__KUBECONFIG_FILENAME__"
              }
          }

        # The default IP Pool to be created for the cluster.
        # Pod IP addresses will be assigned from this pool.
        ippool.yaml: |
            apiVersion: v1
            kind: ipPool
            metadata:
              cidr: 172.1.0.0/16
            spec:
              ipip:
                enabled: true
              nat-outgoing: true

      ---

      # This manifest installs the calico/node container, as well
      # as the Calico CNI plugins and network config on
      # each master and worker node in a Kubernetes cluster.
      kind: DaemonSet
      apiVersion: extensions/v1beta1
      metadata:
        name: calico-node
        namespace: kube-system
        labels:
          k8s-app: calico-node
      spec:
        selector:
          matchLabels:
            k8s-app: calico-node
        template:
          metadata:
            labels:
              k8s-app: calico-node
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
              scheduler.alpha.kubernetes.io/tolerations: |
                [{"key": "dedicated", "value": "master", "effect": "NoSchedule" },
                 {"key":"CriticalAddonsOnly", "operator":"Exists"}]
          spec:
            hostNetwork: true
            containers:
              # Runs calico/node container on each Kubernetes node.  This
              # container programs network policy and routes on each
              # host.
              - name: calico-node
                image: quay.io/calico/node:v1.0.0
                env:
                  # The location of the Calico etcd cluster.
                  - name: ETCD_ENDPOINTS
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_endpoints
                  # Enable BGP.  Disable to enforce policy only.
                  - name: CALICO_NETWORKING
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: enable_bgp
                  # Disable file logging so `kubectl logs` works.
                  - name: CALICO_DISABLE_FILE_LOGGING
                    value: "true"
                  # Don't configure a default pool.  This is done by the Job
                  # below.
                  - name: NO_DEFAULT_POOLS
                    value: "true"
                  # Auto-detect the BGP IP address.
                  - name: IP
                    value: ""
                securityContext:
                  privileged: true
                volumeMounts:
                  - mountPath: /lib/modules
                    name: lib-modules
                    readOnly: true
                  - mountPath: /var/run/calico
                    name: var-run-calico
                    readOnly: false
              # This container installs the Calico CNI binaries
              # and CNI network config file on each node.
              - name: install-cni
                image: calico/cni:v1.5.5
                command: ["/install-cni.sh"]
                env:
                  # The location of the Calico etcd cluster.
                  - name: ETCD_ENDPOINTS
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_endpoints
                  # The CNI network config to install on each node.
                  - name: CNI_NETWORK_CONFIG
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: cni_network_config
                volumeMounts:
                  - mountPath: /host/opt/cni/bin
                    name: cni-bin-dir
                  - mountPath: /host/etc/cni/net.d
                    name: cni-net-dir
            volumes:
              # Used by calico/node.
              - name: lib-modules
                hostPath:
                  path: /lib/modules
              - name: var-run-calico
                hostPath:
                  path: /var/run/calico
              # Used to install CNI.
              - name: cni-bin-dir
                hostPath:
                  path: /opt/cni/bin
              - name: cni-net-dir
                hostPath:
                  path: /etc/cni/net.d

      ---

      # This manifest deploys the Calico policy controller on Kubernetes.
      # See https://github.com/projectcalico/k8s-policy
      apiVersion: extensions/v1beta1
      kind: Deployment
      metadata:
        name: calico-policy-controller
        namespace: kube-system
        labels:
          k8s-app: calico-policy
      spec:
        # The policy controller can only have a single active instance.
        replicas: 1
        strategy:
          type: Recreate
        template:
          metadata:
            name: calico-policy-controller
            namespace: kube-system
            labels:
              k8s-app: calico-policy-controller
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
              scheduler.alpha.kubernetes.io/tolerations: |
                [{"key": "dedicated", "value": "master", "effect": "NoSchedule" },
                 {"key":"CriticalAddonsOnly", "operator":"Exists"}]
          spec:
            # The policy controller must run in the host network namespace so that
            # it isn't governed by policy that would prevent it from working.
            hostNetwork: true
            containers:
              - name: calico-policy-controller
                image: calico/kube-policy-controller:v0.5.1
                env:
                  # The location of the Calico etcd cluster.
                  - name: ETCD_ENDPOINTS
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_endpoints
                  # The location of the Kubernetes API.  Use the default Kubernetes
                  # service for API access.
                  - name: K8S_API
                    value: "https://kubernetes.default:443"
                  # Since we're running in the host namespace and might not have KubeDNS
                  # access, configure the container's /etc/hosts to resolve
                  # kubernetes.default to the correct service clusterIP.
                  - name: CONFIGURE_ETC_HOSTS
                    value: "true"

      ---

      ## This manifest deploys a Job which performs one time
      # configuration of Calico
      apiVersion: batch/v1
      kind: Job
      metadata:
        name: configure-calico
        namespace: kube-system
        labels:
          k8s-app: calico
      spec:
        template:
          metadata:
            name: configure-calico
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ''
              scheduler.alpha.kubernetes.io/tolerations: |
                [{"key": "dedicated", "value": "master", "effect": "NoSchedule" },
                 {"key":"CriticalAddonsOnly", "operator":"Exists"}]
          spec:
            hostNetwork: true
            restartPolicy: OnFailure
            containers:
              # Writes basic configuration to datastore.
              - name: configure-calico
                image: calico/ctl:v1.0.0
                args:
                - apply
                - -f
                - /etc/config/calico/ippool.yaml
                volumeMounts:
                  - name: config-volume
                    mountPath: /etc/config
                env:
                  # The location of the etcd cluster.
                  - name: ETCD_ENDPOINTS
                    valueFrom:
                      configMapKeyRef:
                        name: calico-config
                        key: etcd_endpoints
            volumes:
             - name: config-volume
               configMap:
                 name: calico-config
                 items:
                  - key: ippool.yaml
                    path: calico/ippool.yaml

  - path: /opt/bin/speed
    owner: root
    permissions: "0755"
    content: |
      #!/bin/bash
      iface=$1
      RXB=$(</sys/class/net/"$iface"/statistics/rx_bytes)
      TXB=$(</sys/class/net/"$iface"/statistics/tx_bytes)
      sleep 2
      RXBN=$(</sys/class/net/"$iface"/statistics/rx_bytes)
      TXBN=$(</sys/class/net/"$iface"/statistics/tx_bytes)
      RXDIF=$(echo $((RXBN - RXB)) )
      TXDIF=$(echo $((TXBN - TXB)) )
      echo -e "$((RXDIF / 1024 / 2))K/s $((TXDIF / 1024 / 2))K/s"

  - path: /root/.tmux.conf
    owner: root
    permissions: "0644"
    content: |
      set -g prefix C-a
      bind C-a send-prefix

      bind r source-file ~/.tmux.conf \; display "Reloaded!"

      set -s escape-time 1

      set -g base-index 1
      set -g pane-base-index 1

      set -g default-terminal "screen-256color"
      set -g status-fg colour243
      set -g status-bg colour233
      set -g window-status-fg colour110
      set -g window-status-bg default
      set -g window-status-attr dim

      set -g window-status-current-fg colour110
      set -g window-status-current-bg colour234

      set -g status-left-length 20
      set -g status-left '#[fg=colour243,bg=colour235] Session #S '
      set -g status-right "#[fg=colour136,bg=colour233] net #(speed eth0) #[fg=colour241,bg=colour234] #(/usr/bin/ip route get 8.8.8.8 | awk 'NR==1{print $(NF)}') #[fg=colour244,bg=colour236] %H:%M:%S "
      set -g status-justify left

      # window status
      setw -g window-status-format "#[fg=colour240]#[bg=colour235] #I #[bg=colour233]#[fg=colour240] #W "
      setw -g window-status-current-format "#[bg=colour220]#[fg=colour239] #I-#P #[fg=colour179]#[bg=colour234] #W "

      bind h select-pane -L
      bind j select-pane -D
      bind k select-pane -U
      bind l select-pane -R

      bind -r H resize-pane -L 5
      bind -r J resize-pane -D 5
      bind -r K resize-pane -U 5
      bind -r L resize-pane -R 5

      set-option -g -q mouse on
      setw -g mode-keys vi

      # Setup 'v' to begin selection as in Vim
      bind-key -t vi-copy v begin-selection

      # Update default binding of `Enter` to also use copy-pipe
      unbind -t vi-copy Enter

  - path: /opt/bin/kubectl
    owner: root
    permissions: "0755"
    content: |
      #!/usr/bin/bash -e
      export KUBE_EDITOR=/opt/vim/bin/vim
      if [ ! -f /opt/bin/hyperkube ]; then
      containerid=`docker ps | grep kubelet | grep -v pause | awk '{print $1}'`
        docker cp "$containerid":/hyperkube /opt/bin/
      fi
      exec /opt/bin/hyperkube kubectl "$@"

  - path: /etc/kubelet-docker
    owner: root
    permissions: "0644"
    content: |
      DOCKER_RUN_ARGS="--privileged \
        --pid=host \
        --net=host \
        -v /var/run:/var/run \
        -v /var/lib/kubelet/:/var/lib/kubelet:shared \
        -v /var/lib/docker:/var/lib/docker \
        -v /etc/kubernetes:/etc/kubernetes \
        -v /opt/cni:/opt/cni \
        -v /etc/cni:/etc/cni \
        -v /sys:/sys \
        -e TERM=xterm \
        gcr.io/google_containers/hyperkube-amd64:vK8SVersion"
      KUBELET_ARGS="--network-plugin=cni --pod-manifest-path=/etc/kubernetes/manifests --cluster-dns=10.96.0.10 --cluster-domain=cluster.local --kubeconfig=/etc/kubernetes/kubelet.conf --require-kubeconfig=true --allow-privileged=true --node-labels='storagenode=glusterfs,config-heketi=support' --hostname-override=ServerIPAddress"

  - path: /opt/bin/glusterfs-install.sh
    owner: root
    permissions: "0744"
    content: |
      #!/usr/bin/bash
      PATH=$PATH:/opt/bin
      cd /srv/asset/manifests/glusterfs

      # no need to do anything when true heketi is running
      if kubectl get deployment heketi; then
        exit 0
      fi

      kubectl apply -f heketi-service-account.yaml
      kubectl apply -f glusterfs-daemonset.yaml
      kubectl apply -f deploy-heketi-deployment.yaml
      kubectl apply -f configure-heketi.yaml

  - path: /etc/profile.d/bash_completion.sh
    owner: root
    permissions: "0755"
    content: |
      if ! type kubectl > /dev/null 2>&1; then
          export PATH=/opt/bin:${PATH}
      fi
      
      if [ -e "/var/bash-completion/bash_completion" ]; then
          . /var/bash-completion/bash_completion
          . <(kubectl completion bash)
      fi

  - path: /etc/environment
    owner: root
    permissions: "0644"
    content: |
      PRIVATEIP=ServerIPAddress

  - path: /etc/profile.d/vim.sh
    owner: root
    permissions: "0755"
    content: |
      PATH=/opt/vim/bin:$PATH
      export VIMRUNTIME=/opt/vim/share

  - path: /root/.vimrc
    owner: root
    permissions: "0644"
    content: |
      filetype plugin indent on
      syntax on
      set backspace=indent,eol,start
      set tabstop=2 shiftwidth=2 expandtab autoindent
      set paste
      set listchars=tab:>.,trail:~,extends:>,precedes:<
      set list

  - path: /etc/docker/daemon.json
    owner: root
    permissions: "0644"
    content: |
      {
        "insecure-registries": [ "127.0.0.1:5000","InstallationHostname:32767" ]
      }

  - path: /srv/asset/manifests/registry.yaml
    owner: root
    permissions: "0644"
    content: |
      kind: PersistentVolumeClaim
      apiVersion: v1
      metadata:
        name: kube-registry-pvc
        namespace: kube-system
        labels:
          kubernetes.io/cluster-service: "true"
        annotations:
          volume.beta.kubernetes.io/storage-class: "default"
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 80Gi
  
      ---
      apiVersion: v1
      kind: ReplicationController
      metadata:
        name: kube-registry
        namespace: kube-system
        labels:
          k8s-app: kube-registry
          version: v0
          kubernetes.io/cluster-service: "true"
      spec:
        replicas: 1
        selector:
          k8s-app: kube-registry
          version: v0
        template:
          metadata:
            labels:
              k8s-app: kube-registry
              version: v0
              kubernetes.io/cluster-service: "true"
          spec:
            containers:
            - name: registry
              image: registry:2
              imagePullPolicy: IfNotPresent
              resources:
                limits:
                  cpu: 100m
                  memory: 100Mi
              env:
              - name: REGISTRY_HTTP_ADDR
                value: :5000
              - name: REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY
                value: /var/lib/registry
              volumeMounts:
              - name: is
                mountPath: /var/lib/registry
              ports:
              - containerPort: 5000
                name: registry
                protocol: TCP
            volumes:
            - name: is
              persistentVolumeClaim:
                claimName: kube-registry-pvc
      ---
      apiVersion: v1
      kind: Service
      metadata:
        name: kube-registry
        namespace: kube-system
        labels:
          k8s-app: kube-registry
          kubernetes.io/cluster-service: "true"
          kubernetes.io/name: "KubeRegistry"
      spec:
        selector:
          k8s-app: kube-registry
        type: NodePort
        ports:
        - name: registry
          port: 5000
          protocol: TCP
          nodePort: 32767

coreos:
  etcd2:
    name: etcdserver
    initial-cluster: etcdserver=http://ServerIPAddress:2380
    initial-advertise-peer-urls: http://ServerIPAddress:2380
    advertise-client-urls: http://ServerIPAddress:2379
    # listen on both the official ports and the legacy ports
    # legacy ports can be omitted if your application doesn't depend on them
    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
    listen-peer-urls: http://0.0.0.0:2380

  units:
    - name: systemd-networkd.service
      command: stop
    - name: 00-eth0.network
      runtime: true
      content: |
        [Match]
        Name=eth0

        [Network]
        Address=ServerIPAddress/24
        Gateway=GatewayIP
        DNS=GatewayIP

    - name: down-interfaces.service
      command: start
      content: |
        [Service]
        Type=oneshot
        ExecStart=/usr/bin/ip link set eth0 down
        ExecStart=/usr/bin/ip addr flush dev eth0
    - name: systemd-networkd.service
      command: restart

    - name: prepared.service
      command: start
      content: |
        [Unit]
        Description=Prepare os envirnment
        Requires=network-online.target
        After=network-online.target
        ConditionPathExists=!/.check_prepared.service

        [Service]
        Type=oneshot
        RemainAfterExit=yes
        ExecStartPre=/bin/bash -c "/usr/bin/curl -Lsk http://iPXE_Server_IP/soft/scripts.InstallationHostname.tgz | tar -mzxC /root"
        ExecStart=/bin/bash -exc 'for i in `echo /root/scripts/*.sh`; do cat $i | bash -xe -; done'
        ExecStartPost=/usr/bin/touch /.check_prepared.service

        [Install]
        WantedBy=multi-user.target

    - name: etcd2.service
      command: start

    - name: kubelet.service
      command: start
      content: |
        [Unit]
        Requires=prepared.service
        After=prepared.service

        [Service]
        EnvironmentFile=/etc/kubelet-docker
        Environment=KUBELET_IMAGE_TAG=vK8SVersion_coreos.0
        ExecStartPre=-/usr/bin/docker rm -f kubelet
        #ExecStart=/usr/lib/coreos/kubelet-wrapper $KUBELET_ARGS
        ExecStart=/usr/bin/docker run --name=kubelet --rm $DOCKER_RUN_ARGS ./hyperkube kubelet $KUBELET_ARGS

        KillMode=mixed
        Restart=always
        RestartSec=10

        [Install]
        WantedBy=multi-user.target

    - name: init-k8s-master.service
      command: start
      content: |
        [Unit]
        Description=Initialize k8s mater (http://kubernetes.io/docs/admin/kubeadm/)
        ConditionPathExists=!/etc/kubernetes/kubelet.conf

        [Service]
        Environment=KUBE_HYPERKUBE_IMAGE=gcr.io/google_containers/hyperkube-amd64:vK8SVersion
        ExecStartPre=/usr/bin/ls /.check_prepared.service
        RemainAfterExit=yes
        ExecStart=/opt/bin/kubeadm init --skip-preflight-checks --config=/srv/kubeadm/master.yaml
        Restart=always
        RestartSec=5

        [Install]
        WantedBy=multi-user.target

    - name: taint-master.service
      command: start
      content: |
        [Unit]
        Description=Taint master to K8S

        [Service]
        RemainAfterExit=yes
        ExecStartPre=/bin/bash -c \
            "/opt/bin/kubectl get node -o yaml | grep 'scheduler.alpha.kubernetes.io/taints:.*NoSchedule'"
        ExecStart=/opt/bin/kubectl taint nodes --all dedicated-
        Restart=always
        RestartSec=5

        [Install]
        WantedBy=multi-user.target

    - name: calico-installd.service
      command: start
      content: |
        [Unit]
        ConditionPathExists=!/etc/cni/net.d/10-calico.conf

        [Service]
        RemainAfterExit=yes
        ExecStartPre=/opt/bin/kubectl get po
        ExecStart=/usr/bin/bash -c "PATH=$PATH:/opt/bin && kubectl apply -f /srv/asset/manifests/calico.yaml"
        Restart=always
        RestartSec=5

        [Install]
        WantedBy=multi-user.target

    - name: glusterfs-installd.service
      command: start
      content: |
        [Unit]
        Requires=prepared.service
        After=prepared.service
        Description=Enable gluster heketi

        [Service]
        RemainAfterExit=yes
        ExecStartPre=/opt/bin/kubectl get po
        ExecStart=/opt/bin/glusterfs-install.sh
        Restart=always
        RestartSec=5

        [Install]
        WantedBy=multi-user.target

    - name: registry-installd.service
      command: start
      content: |
        [Unit]
        Description=Initialize docker registry

        [Service]
        RemainAfterExit=yes
        ExecStartPre=/bin/bash -c "/opt/bin/kubectl get storageclass default && ! /opt/bin/kubectl get po -n kube-system | grep kube-registry"
        ExecStart=/opt/bin/kubectl apply -f /srv/asset/manifests/registry.yaml
        Restart=always
        RestartSec=5

        [Install]
        WantedBy=multi-user.target

    - name: hosts.service
      command: start
      content: |
        [Unit]
        Description=Hosts Manager
        Requires=etcd2.service
        After=etcd2.service

        [Service]
        EnvironmentFile=/etc/environment
        Restart=always
        ExecStartPre=/bin/bash -c "etcdctl ls /hosts || etcdctl mkdir /hosts"
        ExecStartPost=/usr/bin/etcdctl set /hosts/%H $PRIVATEIP
        ExecStart=/bin/bash -c 'while true; do \
            echo "127.0.0.1 localhost" > /etc/hosts; \
            for i in $(etcdctl ls /hosts); do \
                echo $(etcdctl get $i) $(echo $i | cut -c 8-); \
            done >> /etc/hosts; \
            sleep 5;\
          done'
        ExecStopPost=/usr/bin/etcdctl rm /hosts/%H

        [Install]
        WantedBy=multi-user.target

users:
  - name: andrew
    passwd: $1$cxDrcmk0$B/WI7rG8E4THMzpkyPe.q.
    groups:
      - sudo
      - docker
  - name: inu
    passwd: "$6$rounds=4096$yTXdzXEJJzKRX74$Nxo6drS5qdJVpV5C.RqVE2ufRwqq./62xrKRXws4tWI0KM1jMPjt.DbESTdDdzHMYIbbkMeGUQa6j4Ow0Rar7."
    groups:
      - sudo
      - docker
  - name: henry
    passwd: "$6$rounds=4096$yTXdzXEJJzKRX74$Nxo6drS5qdJVpV5C.RqVE2ufRwqq./62xrKRXws4tWI0KM1jMPjt.DbESTdDdzHMYIbbkMeGUQa6j4Ow0Rar7."
    groups:
      - sudo
      - docker
